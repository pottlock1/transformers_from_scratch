{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5118e708-2195-4a4b-a769-28e89c747153",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "- Converting strings to integers so that language models can read them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e73cbb-5613-41bd-8674-5b1d5993c694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185561"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = open(\"../data/taylorswift.txt\", \"r\").read()\n",
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a01860-373a-4030-bcae-3528694272ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"#$&\\'()+,-./0123456789:;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyzÂ£Â®Ã¡Ã©Ã­Ã±Ã¶â€“â€”â€¢â„¢'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unique_chars = ''.join(sorted(list(set(train_text))))\n",
    "all_unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2369b910-a769-41e6-a321-98805e8aae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s : i for i, s in enumerate(all_unique_chars)}\n",
    "itos = {i : s for i, s in enumerate(all_unique_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe67a54-a117-4b71-a67f-afe9da5ca9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = lambda s: [stoi[chr] for chr in s]\n",
    "decoder = lambda d: \"\".join([itos[i] for i in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87c1d8b-b0b1-4158-8465-e8f3249d878c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder(\"Hi there\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20dbba-79b7-44ee-8262-64dc70cfa79e",
   "metadata": {},
   "source": [
    "# Building BPE tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c9258b-917f-4d87-a2d9-02eab83a6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0ae11-b267-468c-bd32-b11349918ee8",
   "metadata": {},
   "source": [
    "# Unicode code points\n",
    "- Unicode is a world dictionary to represent any chararcter into integer\n",
    "- ASCII is a subset of Unicode code points\n",
    "- Only one integer for one character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c09059-1545-4c4b-8b1a-b445ce5806ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[67, 111, 112, 121, 32, 112, 97, 115, 116, 101]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in train_text[:10]] #--> uni code codepoints for each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fad3498-bc74-4e98-bb4d-cfb38fcac3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128516"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('ðŸ˜„')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94c851fe-d3da-47f6-8914-4f8325ecbb94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ˜„'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(128516)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93983e2f-baf3-43ca-9625-8ecb8553c63f",
   "metadata": {},
   "source": [
    "## Drawbacks\n",
    "- Pretty much alive in terms of update\n",
    "- Have more than 1M+ characters in it\n",
    "- Causing embedding table too huge to train\n",
    "- Hence not recommended to use directly for tokenization\n",
    "- Ref: https://en.wikipedia.org/wiki/Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab0608-2c74-422b-b352-877ed289999f",
   "metadata": {},
   "source": [
    "# UTF Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea164a23-b45c-4245-adb1-4992090a28af",
   "metadata": {},
   "source": [
    "- There multi utf encodings like utf-8, utf-16, utf-32.\n",
    "- We will discuss utf-8, famously used for various LLMs\n",
    "- Converts each code points into its raw bytes\n",
    "- Can go upto 4 bytes\n",
    "- 1 byte = 8 bits (number ranging from 0-255)\n",
    "- Common ASCII characters like a-zA-Z 0-9 and somes symbols take 1 byte\n",
    "- characters like 'Ã©' take 2 bytes\n",
    "- Some language's characters take 3 bytes like Hindi, Chinese, Arabic etc\n",
    "- Emojis take 4 bytes to store in the machine\n",
    "- Ref: https://en.wikipedia.org/wiki/UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f8f8b4b-95a9-4061-9ff2-f892715bfcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('a'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb178dc6-f5d1-49c6-99c5-2fbb9de2a0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[195, 169]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list('Ã©'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1391996b-4a32-4aca-9a36-b34156360d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[224, 164, 168]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(('à¤¨').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d06e9a77-e3f0-40c8-beab-6a689ddfafb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[240, 159, 152, 132]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(('ðŸ˜„').encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f14637b2-3471-4d13-b2e0-a901ec202c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[224, 164, 168, 224, 164, 174, 224, 164, 184, 224, 165, 141, 224, 164, 164, 224, 165, 135, 44, 32, 224, 164, 134, 224, 164, 170, 32, 224, 164, 149, 224, 165, 136, 224, 164, 184, 224, 165, 135, 32, 224, 164, 185, 224, 165, 136, 224, 164, 130]\n"
     ]
    }
   ],
   "source": [
    "# Encoding the whole word\n",
    "print(list(('à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚').encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b107de6d-7d25-458c-8883-10987609682b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding\n",
    "b = list(('à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚').encode('utf-8'))\n",
    "bytes(b).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae5c74-1340-4bd5-96be-0ccea298b628",
   "metadata": {},
   "source": [
    "## Drawbacks\n",
    "- Does not make any sense semantically\n",
    "- Have only 0 - 255 tokens in vocabulary\n",
    "- Not every sequence of bytes is valid UTF-8 encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71776668-ce1b-4aec-a1a3-dce3730997a4",
   "metadata": {},
   "source": [
    "# Byte Pair Encodings\n",
    "- https://en.wikipedia.org/wiki/Byte-pair_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f45f7b95-0456-4190-9420-444c207ffcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of string chars: 185561\n",
      "Lenght of utf-8 bytes: 185768\n"
     ]
    }
   ],
   "source": [
    "ids = train_text.encode('utf-8')\n",
    "print(\"Length of string chars:\", len(train_text))\n",
    "print(\"Lenght of utf-8 bytes:\", len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "033546d2-05fe-4049-9b9f-d2bf38ccf4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids: List) -> dict:\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary to give the count of the pairs coming consecutively\n",
    "    \"\"\"\n",
    "    stats_dict = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        stats_dict[pair] = stats_dict.get(pair, 0) + 1\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2ad8c7d-bb41-465b-9d86-aeaa2ff9c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = get_stats(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7edd061b-e3e3-41c1-a4a6-e01239df9c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101, 32), (10, 45))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(stats_dict, key=stats_dict.get), min(stats_dict, key=stats_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90e3ebb1-5685-4abc-8eca-0edf740ffe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2981"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_dict[(101, 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07c39f99-a980-4a31-adbb-ccf4ecb262aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, new_idx):\n",
    "    \"\"\"Replace the pair at all the places with the new index\n",
    "    \"\"\"\n",
    "    if len(ids) == 1:\n",
    "        return ids\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            new_ids.append(new_idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36284280-4966-491f-96e5-c7259cafecdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 11, 3, 11]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [1, 2, 1, 2, 3, 1, 2]\n",
    "merge(ids, (1, 2), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e98a4a43-9e15-4399-aec1-7704b329e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw bytes: 185768\n",
      "Length of merged bytes: 58300\n"
     ]
    }
   ],
   "source": [
    "n_vocab = 1000\n",
    "new_merges = n_vocab - 256\n",
    "train_raw_bytes = list(train_text.encode('utf-8'))\n",
    "print(f\"Length of raw bytes: {len(train_raw_bytes)}\")\n",
    "i = 0\n",
    "merge_dict = {}\n",
    "while i < new_merges:\n",
    "    stats_dict = get_stats(train_raw_bytes)\n",
    "    top_pair = max(stats_dict, key = stats_dict.get)\n",
    "    new_token = 256 + i\n",
    "    train_raw_bytes = merge(train_raw_bytes, top_pair, 256 + i)\n",
    "    merge_dict[new_token] = top_pair\n",
    "    i += 1\n",
    "print(f\"Length of merged bytes: {len(train_raw_bytes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72142340-1422-4922-bd48-f40a6d41f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "s = \"Hi there, how are you? à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚\"\n",
    "def encode(s):\n",
    "    s_raw_bytes = list(s.encode('utf-8'))\n",
    "    print(f\"Length of raw bytes: {len(s_raw_bytes)}\")\n",
    "    for idx in merge_dict:\n",
    "        s_raw_bytes = merge(s_raw_bytes, merge_dict[idx], idx)\n",
    "    print(f\"Length of final merged bytes: {len(s_raw_bytes)}\")\n",
    "    return s_raw_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "894c7ee4-dd78-417a-9ae3-3ebb719b1f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw bytes: 72\n",
      "Length of final merged bytes: 61\n"
     ]
    }
   ],
   "source": [
    "s_raw_bytes = encode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f90885c-c74f-49e0-80b3-570109960189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode\n",
    "vocab = {id : bytes([id]) for id in range(256)}\n",
    "for idx in merge_dict:\n",
    "    vocab[idx] = vocab[merge_dict[idx][0]] + vocab[merge_dict[idx][1]]\n",
    "def decode(ids):\n",
    "    bts = b\"\".join([vocab[i] for i in ids])\n",
    "    bts = bts.decode('utf-8', errors = \"replace\")\n",
    "    return bts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fea55b9-642a-4a51-a452-f65bde156e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there, how are you? à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(s_raw_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec8464e8-21cd-4b12-9e05-cefc4962d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw bytes: 1\n",
      "Length of final merged bytes: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c95ac67-38c0-4f06-8a4a-b1af60a29491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to save merge_dict and vocab objects\n",
    "with open(\"merge_dict.json\", 'w') as file:\n",
    "    json.dump(merge_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead7068-6bdd-40e9-aca4-a9e9b0d79f32",
   "metadata": {},
   "source": [
    "# Final code for BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e64e7ba0-9c46-42b8-8030-691977a586db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.merge_dict = None\n",
    "        self.vocab = None\n",
    "\n",
    "    def _get_stats(self, ids: List, stats_dict = None) -> dict:\n",
    "        \"\"\"\n",
    "        Given a list of integers, return a dictionary to give the count of the pairs coming consecutively\n",
    "        \"\"\"\n",
    "        stats_dict = {} if stats_dict is None else stats_dict\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "            stats_dict[pair] = stats_dict.get(pair, 0) + 1\n",
    "        return stats_dict\n",
    "    \n",
    "    def _merge(self, ids, pair, new_idx):\n",
    "        \"\"\"Replace the pair at all the places with the new index\n",
    "        \"\"\"\n",
    "        if len(ids) == 1:\n",
    "            return ids\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "                new_ids.append(new_idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i += 1\n",
    "        return new_ids\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        \"\"\"Function builds the vocab dict mapping each token to its raw bytes\"\"\"\n",
    "        self.vocab = {id : bytes([id]) for id in range(256)}\n",
    "        for idx in self.merge_dict:\n",
    "            # print(self.merge_dict[idx])\n",
    "            self.vocab[idx] = self.vocab[self.merge_dict[idx][0]] + self.vocab[self.merge_dict[idx][1]]\n",
    "            # print(self.vocab)\n",
    "        print(f\"Vocabulary has been built internally of length {len(self.vocab)}, ready to encode and decode\")\n",
    "        \n",
    "    def train(self, train_text: str, n_vocab : int, merge_dict_name = \"merge_dict\") -> dict:\n",
    "        \"\"\"Function will take a train_text on which the BPE tokenizer will get trained\n",
    "        Parameters:\n",
    "        train_text: single python string\n",
    "        n_vocab: size of the vocabulary to be built\n",
    "        \"\"\"\n",
    "        new_merges = n_vocab - 256\n",
    "        train_raw_bytes = list(train_text.encode('utf-8'))\n",
    "        print(f\"Length of raw bytes: {len(train_raw_bytes)}\")\n",
    "        i = 0\n",
    "        merge_dict = {}\n",
    "        while i < new_merges:\n",
    "            stats_dict = self._get_stats(train_raw_bytes)\n",
    "            top_pair = max(stats_dict, key = stats_dict.get)\n",
    "            new_token = 256 + i\n",
    "            train_raw_bytes = self._merge(train_raw_bytes, top_pair, 256 + i)\n",
    "            merge_dict[new_token] = top_pair\n",
    "            i += 1\n",
    "        print(f\"Length of merged bytes: {len(train_raw_bytes)}\")\n",
    "        # need to save merge_dict\n",
    "        merge_dict_path = f\"{merge_dict_name}.json\"\n",
    "        with open(merge_dict_path, 'w') as file:\n",
    "            json.dump(merge_dict, file)\n",
    "        print(f\"Merge dict has been save on the path: {merge_dict_path}\")\n",
    "        self.merge_dict = merge_dict\n",
    "        self._build_vocab()\n",
    "\n",
    "    def from_pretrained(self, merge_dict_path):\n",
    "        with open(merge_dict_path, \"r\") as file:\n",
    "            self.merge_dict = json.load(file)\n",
    "        # When laoding a saved json, all the object's key gets converted into string\n",
    "        self.merge_dict = {int(key) : val for key, val in self.merge_dict.items()}\n",
    "        self._build_vocab()\n",
    "    \n",
    "    def encode(self, s : str) -> List[int]:\n",
    "        \"\"\"Function takes a single string and encodes it into bytes\"\"\"\n",
    "        s_raw_bytes = list(s.encode('utf-8'))\n",
    "        print(f\"Length of raw bytes: {len(s_raw_bytes)}\")\n",
    "        for idx in self.merge_dict:\n",
    "            s_raw_bytes = self._merge(s_raw_bytes, self.merge_dict[idx], idx)\n",
    "        print(f\"Length of final merged bytes: {len(s_raw_bytes)}\")\n",
    "        return s_raw_bytes\n",
    "    \n",
    "    def decode(self, ids : List[int]) -> Union[str, List[str]]:\n",
    "        \"\"\"Function will take a sequence of bytes and decode it back to unicode codepoints\"\"\"\n",
    "        if self.vocab is not None:\n",
    "            bts = b\"\".join([self.vocab[i] for i in ids])\n",
    "            bts = bts.decode('utf-8', errors = \"replace\")\n",
    "            return bts\n",
    "        else:\n",
    "            raise ValueError(\"Vocab has not been built, please built it first and then call decode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d9679de-d55b-4af3-abf5-5ff77557c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1870fd9-e644-4c48-a7f4-10ce12defc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185561\n",
      "Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\n",
      "---\n",
      "\n",
      "Main menu\n",
      "\n",
      "WikipediaTh\n"
     ]
    }
   ],
   "source": [
    "taylor_swift_train_data = open(\"./minbpe/tests/taylorswift.txt\", \"r\").read()\n",
    "print(len(taylor_swift_train_data))\n",
    "print(taylor_swift_train_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "204ff69f-597c-439a-8c8c-7caea0d2b701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw bytes: 185768\n",
      "Length of merged bytes: 45722\n",
      "Merge dict has been save on the path: merge_dict_taylor_swift.json\n",
      "Vocabulary has been built internally of length 2000, ready to encode and decode\n",
      "CPU times: user 12.9 s, sys: 3.05 ms, total: 12.9 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.train(taylor_swift_train_data, 2000, \"merge_dict_taylor_swift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dadc0248-b87e-4ffd-8913-3e143d8ab27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has been built internally of length 2000, ready to encode and decode\n"
     ]
    }
   ],
   "source": [
    "tokenizer.from_pretrained(\"./merge_dict_taylor_swift.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c0bdecc-36af-4d58-bd7c-61ce6a3c9b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw bytes: 49\n",
      "Length of final merged bytes: 25\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Hi there, how are yopu. I havnt seen you for days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb60d045-45a1-495e-ae14-9a0614375e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there, how are yopu. I havnt seen you for days'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fef4f-53bf-414a-b8d9-3d488ac0c99a",
   "metadata": {},
   "source": [
    "# See the merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a18f6302-5807-4aea-83f8-874a74023e4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'n'] --> b'an'\n",
      "['a', 'r'] --> b'ar'\n",
      "['e', 'r', ' '] --> b'er '\n",
      "['y', ' '] --> b'y '\n",
      "['a', 'l'] --> b'al'\n",
      "['t', 'h', 'e', ' '] --> b'the '\n",
      "['v', 'e', 'd', ' '] --> b'ved '\n",
      "['w', 'i'] --> b'wi'\n",
      "['e', 'r'] --> b'er'\n",
      "['o', 'n', ' '] --> b'on '\n",
      "['w', 'i', 'f'] --> b'wif'\n",
      "['R', 'e'] --> b'Re'\n",
      "['S', 'w', 'i', 'f'] --> b'Swif'\n",
      "['o', 'r', ' '] --> b'or '\n",
      "['c', 'h'] --> b'ch'\n",
      "[',', ' ', '2', '0', '1'] --> b', 201'\n",
      "['o', 'm'] --> b'om'\n",
      "['b', 'e', 'r', ' '] --> b'ber '\n",
      "[' ', 't', 'h', 'e', ' '] --> b' the '\n",
      "['a', 'y'] --> b'ay'\n",
      "['e', 'n'] --> b'en'\n",
      "['o', 'r'] --> b'or'\n",
      "['a', 'l', ' '] --> b'al '\n",
      "['e', 'm'] --> b'em'\n",
      "['.', '\\n'] --> b'.\\n'\n",
      "['r', 'i', 'e'] --> b'rie'\n",
      "['i', 'n', 'g'] --> b'ing'\n",
      "[',', ' ', '2', '0', '2'] --> b', 202'\n",
      "['t', 'i'] --> b'ti'\n",
      "['a', 'y', 'l'] --> b'ayl'\n",
      "['\"', '.', ' '] --> b'\". '\n",
      "['l', 'l'] --> b'll'\n",
      "['T', 'a', 'y', 'l'] --> b'Tayl'\n",
      "['t', 'r', 'i', 'e'] --> b'trie'\n",
      "['.', '\\n', ' '] --> b'.\\n '\n",
      "['t', 'o'] --> b'to'\n",
      "['.', ' ', 'R', 'e'] --> b'. Re'\n",
      "['.', ' ', 'R', 'e', 't', 'r', 'i', 'e'] --> b'. Retrie'\n",
      "['.', ' ', 'R', 'e', 't', 'r', 'i', 'e', 'v', 'e', 'd', ' '] --> b'. Retrieved '\n",
      "['T', 'a', 'y', 'l', 'o', 'r', ' '] --> b'Taylor '\n",
      "['e', 's'] --> b'es'\n",
      "['T', 'a', 'y', 'l', 'o', 'r', ' ', 'S', 'w', 'i', 'f'] --> b'Taylor Swif'\n",
      "['u', 's'] --> b'us'\n",
      "['r', 'o', 'm'] --> b'rom'\n",
      "['e', 'm', 'b', 'e', 'r', ' '] --> b'ember '\n",
      "[')', '.', ' '] --> b'). '\n",
      "['A', 'r'] --> b'Ar'\n",
      "['f', 'r', 'o', 'm'] --> b'from'\n",
      "[')', '.', ' ', '\"'] --> b'). \"'\n",
      "['a', 'n', 'd', ' '] --> b'and '\n",
      "['r', 'e'] --> b're'\n",
      "['o', 'u'] --> b'ou'\n",
      "['o', 'r', 'i'] --> b'ori'\n",
      "['o', 'f'] --> b'of'\n",
      "['g', 'i', 'n'] --> b'gin'\n",
      "['i', 'n', 'g', ' '] --> b'ing '\n",
      "['c', 'h', 'i'] --> b'chi'\n",
      "[']', ' '] --> b'] '\n",
      "['g', 'i', 'n', 'a', 'l', ' '] --> b'ginal '\n",
      "['f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' '] --> b'from the '\n",
      "['o', 'r', 'i', 'g', 'i', 'n', 'a', 'l', ' '] --> b'original '\n",
      "['h', 'e', ' '] --> b'he '\n",
      "['A', 'r', 'c', 'h', 'i'] --> b'Archi'\n",
      "['A', 'r', 'c', 'h', 'i', 'v', 'e', 'd', ' '] --> b'Archived '\n",
      "['f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 'o', 'r', 'i', 'g', 'i', 'n', 'a', 'l', ' '] --> b'from the original '\n",
      "['A', 'r', 'c', 'h', 'i', 'v', 'e', 'd', ' ', 'f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 'o', 'r', 'i', 'g', 'i', 'n', 'a', 'l', ' '] --> b'Archived from the original '\n",
      "['A', 'r', 'c', 'h', 'i', 'v', 'e', 'd', ' ', 'f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 'o', 'r', 'i', 'g', 'i', 'n', 'a', 'l', ' ', 'o', 'n', ' '] --> b'Archived from the original on '\n",
      "['.', ' ', 'A', 'r', 'c', 'h', 'i', 'v', 'e', 'd', ' ', 'f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 'o', 'r', 'i', 'g', 'i', 'n', 'a', 'l', ' ', 'o', 'n', ' '] --> b'. Archived from the original on '\n",
      "['a', ' '] --> b'a '\n",
      "['s', 't'] --> b'st'\n",
      "['i', 'c'] --> b'ic'\n",
      "['.', '['] --> b'.['\n",
      "['e', 'c'] --> b'ec'\n",
      "['i', 'l', 'l'] --> b'ill'\n",
      "[\"'\", 's', ' '] --> b\"'s \"\n",
      "['T', 'a', 'y', 'l', 'o', 'r', ' ', 'S', 'w', 'i', 'f', 't', ' '] --> b'Taylor Swift '\n",
      "['o', 'v'] --> b'ov'\n",
      "['a', 't'] --> b'at'\n",
      "['a', 's', ' '] --> b'as '\n",
      "['e', 's', ' '] --> b'es '\n",
      "['J', 'u'] --> b'Ju'\n",
      "['o', 'f', ' '] --> b'of '\n",
      "['t', 'o', ' '] --> b'to '\n",
      "['u', 'm'] --> b'um'\n",
      "['T', 'h', 'e', ' '] --> b'The '\n",
      "['a', 'r', 'd'] --> b'ard'\n",
      "['i', 'n', ' '] --> b'in '\n",
      "['a', 'n', ' '] --> b'an '\n",
      "['e', 'l'] --> b'el'\n",
      "[',', ' ', '2', '0', '2', '3'] --> b', 2023'\n",
      "['a', 'r', 'y', ' '] --> b'ary '\n",
      "['t', 'h', ' '] --> b'th '\n",
      "['a', 'm'] --> b'am'\n",
      "['l', 'y', ' '] --> b'ly '\n",
      "['o', 'p'] --> b'op'\n",
      "['T', 'a', 'y', 'l', 'o', 'r', ' ', 'S', 'w', 'i', 'f', 't'] --> b'Taylor Swift'\n",
      "['t', 'r'] --> b'tr'\n",
      "['i', 's'] --> b'is'\n",
      "['h', 'e', 'r', ' '] --> b'her '\n",
      "['o', ' '] --> b'o '\n",
      "['u', 'a', 'r', 'y', ' '] --> b'uary '\n",
      "['N', 'o', 'v'] --> b'Nov'\n",
      "['u', 's', 'i', 'c'] --> b'usic'\n",
      "['N', 'o', 'v', 'e', 'm', 'b', 'e', 'r', ' '] --> b'November '\n",
      "['e', 'w'] --> b'ew'\n",
      "['a', 't', ' '] --> b'at '\n",
      "['l', ' '] --> b'l '\n",
      "[':', ' '] --> b': '\n",
      "['b', 'o'] --> b'bo'\n",
      "['S', 'w', 'i', 'f', 't', ' '] --> b'Swift '\n",
      "['D', 'e', 'c'] --> b'Dec'\n",
      "['i', 't'] --> b'it'\n",
      "['i', 'g'] --> b'ig'\n",
      "['B', 'i', 'l', 'l'] --> b'Bill'\n",
      "['1', '0'] --> b'10'\n",
      "['a', 's'] --> b'as'\n",
      "['o', 'n', 'g'] --> b'ong'\n",
      "['O', 'c'] --> b'Oc'\n",
      "['a', 't', 'i'] --> b'ati'\n",
      "['S', 't'] --> b'St'\n",
      "['O', 'c', 't', 'o'] --> b'Octo'\n",
      "['O', 'c', 't', 'o', 'b', 'e', 'r', ' '] --> b'October '\n",
      "['a', 'c'] --> b'ac'\n",
      "['o', 'w'] --> b'ow'\n",
      "['D', 'e', 'c', 'e', 'm', 'b', 'e', 'r', ' '] --> b'December '\n",
      "['B', 'i', 'l', 'l', 'b', 'o'] --> b'Billbo'\n",
      "['a', 'd'] --> b'ad'\n",
      "['l', 'e'] --> b'le'\n",
      "['u', 'r'] --> b'ur'\n",
      "['f', 'o', 'r', ' '] --> b'for '\n",
      "[' ', '('] --> b' ('\n",
      "[',', ' ', '2', '0', '2', '2'] --> b', 2022'\n",
      "['u', 'g'] --> b'ug'\n",
      "['c', 'h', ' '] --> b'ch '\n",
      "['s', 't', ' '] --> b'st '\n",
      "['o', 'u', 'n'] --> b'oun'\n",
      "['b', 'u', 'm'] --> b'bum'\n",
      "['o', 'l'] --> b'ol'\n",
      "['u', 's', 't', ' '] --> b'ust '\n",
      "['e', 'b'] --> b'eb'\n",
      "['M', 'a'] --> b'Ma'\n",
      "['J', 'u', 'l', 'y', ' '] --> b'July '\n",
      "[')', '.', ' ', '\"', 'T', 'a', 'y', 'l', 'o', 'r', ' ', 'S', 'w', 'i', 'f', 't', ' '] --> b'). \"Taylor Swift '\n",
      "['k', ' '] --> b'k '\n",
      "['e', 'r', 's'] --> b'ers'\n",
      "[']', '['] --> b']['\n",
      "['A', 'u', 'g'] --> b'Aug'\n",
      "['A', 'u', 'g', 'u', 's', 't', ' '] --> b'August '\n",
      "['i', 'd'] --> b'id'\n",
      "[',', ' ', '2', '0', '2', '1'] --> b', 2021'\n",
      "['m', 'e'] --> b'me'\n",
      "['e', 'p'] --> b'ep'\n",
      "['2', '0', '1'] --> b'201'\n",
      "['2', '3'] --> b'23'\n",
      "[',', ' ', '2', '0', '1', '2'] --> b', 2012'\n",
      "['e', 'a', 'r'] --> b'ear'\n",
      "[',', ' ', '2', '0', '2', '0'] --> b', 2020'\n",
      "['I', 'n'] --> b'In'\n",
      "['f', 'i'] --> b'fi'\n",
      "['n', 'e', ' '] --> b'ne '\n",
      "['B', 'i', 'l', 'l', 'b', 'o', 'a', 'r', 'd'] --> b'Billboard'\n",
      "['r', 'i', 't'] --> b'rit'\n",
      "['h', 'i'] --> b'hi'\n",
      "['u', 's', 'i', 'c', ' '] --> b'usic '\n",
      "['.', '\\n', ' ', '\"'] --> b'.\\n \"'\n",
      "['N', 'e', 'w'] --> b'New'\n",
      "['d', 'i'] --> b'di'\n",
      "['A', 'p'] --> b'Ap'\n",
      "[',', ' ', '2', '0', '1', '9'] --> b', 2019'\n",
      "['r', 'o'] --> b'ro'\n",
      "[\"'\", ' '] --> b\"' \"\n",
      "['s', ',', ' '] --> b's, '\n",
      "['J', 'u', 'n', 'e', ' '] --> b'June '\n",
      "['o', 'f', ' ', 't', 'h', 'e', ' '] --> b'of the '\n",
      "['c', 'o', 'r'] --> b'cor'\n",
      "['2', '1'] --> b'21'\n",
      "['1', '9'] --> b'19'\n",
      "['i', 'm'] --> b'im'\n",
      "['e', 'n', ' '] --> b'en '\n",
      "['e', 'b', 'r'] --> b'ebr'\n",
      "['e', 'n', 't'] --> b'ent'\n",
      "['o', 'l', 'l'] --> b'oll'\n",
      "['M', 'a', 'r'] --> b'Mar'\n",
      "['r', 'i', 'c'] --> b'ric'\n",
      "['w', 'i', 't', 'h', ' '] --> b'with '\n",
      "[',', '['] --> b',['\n",
      "['F', 'e', 'b', 'r'] --> b'Febr'\n",
      "['F', 'e', 'b', 'r', 'u', 'a', 'r', 'y', ' '] --> b'February '\n",
      "['T', 'a', 'y', 'l', 'o', 'r', ' ', 'S', 'w', 'i', 'f', 't', \"'\", 's', ' '] --> b\"Taylor Swift's \"\n",
      "['\"', '.', ' ', 'B', 'i', 'l', 'l', 'b', 'o', 'a', 'r', 'd'] --> b'\". Billboard'\n",
      "['e', 'a'] --> b'ea'\n",
      "[',', ' ', '2', '0', '1', '6'] --> b', 2016'\n",
      "['e', 'p', 't'] --> b'ept'\n",
      "['M', 'a', 'y', ' '] --> b'May '\n",
      "[',', ' ', '2', '0', '1', '5'] --> b', 2015'\n",
      "['A', 'p', 'r', 'i'] --> b'Apri'\n",
      "['A', 'p', 'r', 'i', 'l', ' '] --> b'April '\n",
      "['l', 'e', ' '] --> b'le '\n",
      "['A', 'w'] --> b'Aw'\n",
      "['a', 't', 'i', 'o', 'n'] --> b'ation'\n",
      "['S', 'e', 'p', 't'] --> b'Sept'\n",
      "['S', 'e', 'p', 't', 'e', 'm', 'b', 'e', 'r', ' '] --> b'September '\n",
      "['r', 'a'] --> b'ra'\n",
      "['a', 'l', 'b', 'u', 'm'] --> b'album'\n",
      "['C', 'h'] --> b'Ch'\n",
      "['v', 'e', ' '] --> b've '\n",
      "['e', 's', 't', ' '] --> b'est '\n",
      "['J', 'a', 'n'] --> b'Jan'\n",
      "['2', '2'] --> b'22'\n",
      "['J', 'a', 'n', 'u', 'a', 'r', 'y', ' '] --> b'January '\n",
      "['o', 'u', 'n', 't', 'r'] --> b'ountr'\n",
      "['i', 'g', 'h'] --> b'igh'\n",
      "['\"', '.', ' ', 'T', 'h', 'e', ' '] --> b'\". The '\n",
      "[',', ' ', '2', '0', '2', '3', '.', '\\n', ' '] --> b', 2023.\\n '\n",
      "['1', '3'] --> b'13'\n",
      "['A', 'l'] --> b'Al'\n",
      "['e', 't'] --> b'et'\n",
      "['e', 's', 's'] --> b'ess'\n",
      "['M', 'a', 'r', 'c', 'h', ' '] --> b'March '\n",
      "['u', 't'] --> b'ut'\n",
      "['w', 'r', 'i', 't'] --> b'writ'\n",
      "['l', 'o'] --> b'lo'\n",
      "['s', 'o', 'n', 'g'] --> b'song'\n",
      "['ï¿½', 'ï¿½'] --> b'\\xe2\\x80'\n",
      "['a', 'r', 'd', ' '] --> b'ard '\n",
      "['0', ' '] --> b'0 '\n",
      "['u', 'l'] --> b'ul'\n",
      "['2', '4'] --> b'24'\n",
      "['i', 's', ' '] --> b'is '\n",
      "['t', 'i', 'c'] --> b'tic'\n"
     ]
    }
   ],
   "source": [
    "for n in range(270, 500):\n",
    "    b_final = tokenizer.merge_dict[n].copy()\n",
    "    while any([x >= 256 for x in b_final]):\n",
    "        b_final_new = []\n",
    "        for b in b_final:\n",
    "            if b >= 256:\n",
    "                b_final_new.extend(tokenizer.merge_dict[b])\n",
    "            else:\n",
    "                b_final_new.append(b)\n",
    "        b_final = b_final_new\n",
    "    print(f\"{[bytes([b]).decode('utf-8', errors='replace') for b in b_final]} --> {tokenizer.vocab[n]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2ab17-f2ef-4ce2-8a26-88c0991f26f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e46c8b-69bb-44ee-a472-4d71794d5588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd6f0f-9701-44c8-a92b-6a027bd17958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159a391-5b55-44ca-83c2-875f55da1917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51aaa6c-8b7b-4ea4-9aee-32a2bdaf33dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0268f-3083-450f-b81d-15576aee2bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f1fd5f-cea2-4f47-8151-7845dd3b7f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37399ae5-311b-4c32-811f-9fe498894a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e2c69-ddae-42b2-a441-172f3c32689f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7d49f-f829-4fc4-9cc9-03e06b44e52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179d708-043d-4550-a695-0143d9ee864f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf86bf-bcf2-42d3-81df-b77696e7d81b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
